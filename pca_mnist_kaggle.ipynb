{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0b4909e",
   "metadata": {},
   "source": [
    "# PCA for MNIST Digit Classification using Decision Tree\n",
    "\n",
    "This notebook demonstrates Principal Component Analysis (PCA) for dimensionality reduction and classification of MNIST handwritten digits using a Decision Tree classifier. All visualizations are displayed inline for easy interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077cd705",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c8173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e2f8a9",
   "metadata": {},
   "source": [
    "## Section 2: Load and Explore MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d47ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST-like dataset (scikit-learn digits dataset - 8x8 images of digits 0-9)\n",
    "digits = load_digits()\n",
    "X = digits.data  # 1797 samples, 64 features (8x8 pixels)\n",
    "y = digits.target  # labels 0-9\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MNIST DATASET OVERVIEW\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Number of features (pixels): {X.shape[1]}\")\n",
    "print(f\"Number of classes (digits): {len(np.unique(y))}\")\n",
    "print(f\"Classes: {np.unique(y)}\")\n",
    "print(f\"Feature value range: [{X.min():.2f}, {X.max():.2f}]\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dadfa9",
   "metadata": {},
   "source": [
    "## Section 3: Standardize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e647e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"\\nData Standardization Completed\")\n",
    "print(f\"Mean of scaled data: {X_scaled.mean():.6f}\")\n",
    "print(f\"Std of scaled data: {X_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ed100",
   "metadata": {},
   "source": [
    "## Section 4: Apply PCA and Analyze Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5daf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA with all components\n",
    "pca_full = PCA(n_components=64)\n",
    "X_pca_full = pca_full.fit_transform(X_scaled)\n",
    "\n",
    "# Get explained variance ratio\n",
    "explained_variance_ratio = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PCA ANALYSIS - EXPLAINED VARIANCE BY COMPONENT\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nFirst 10 Principal Components:\")\n",
    "for i in range(10):\n",
    "    print(f\"PC{i+1:2d}: {explained_variance_ratio[i]:.6f} ({explained_variance_ratio[i]*100:6.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CUMULATIVE EXPLAINED VARIANCE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"First 5 components  : {cumulative_variance[4]:.6f} ({cumulative_variance[4]*100:6.2f}%)\")\n",
    "print(f\"First 10 components : {cumulative_variance[9]:.6f} ({cumulative_variance[9]*100:6.2f}%)\")\n",
    "print(f\"First 15 components : {cumulative_variance[14]:.6f} ({cumulative_variance[14]*100:6.2f}%)\")\n",
    "print(f\"First 20 components : {cumulative_variance[19]:.6f} ({cumulative_variance[19]*100:6.2f}%)\")\n",
    "print(f\"First 30 components : {cumulative_variance[29]:.6f} ({cumulative_variance[29]*100:6.2f}%)\")\n",
    "print(f\"First 50 components : {cumulative_variance[49]:.6f} ({cumulative_variance[49]*100:6.2f}%)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf86af87",
   "metadata": {},
   "source": [
    "## Section 5: Visualize Scree Plot and Cumulative Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c919575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Scree plot and cumulative variance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Scree plot\n",
    "ax1.plot(range(1, 65), explained_variance_ratio, 'bo-', linewidth=2, markersize=6)\n",
    "ax1.set_xlabel('Principal Component', fontsize=12)\n",
    "ax1.set_ylabel('Explained Variance Ratio', fontsize=12)\n",
    "ax1.set_title('Scree Plot - MNIST Dataset', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(0, 65)\n",
    "\n",
    "# Cumulative explained variance\n",
    "ax2.plot(range(1, 65), cumulative_variance*100, 'ro-', linewidth=2, markersize=6, label='Cumulative Variance')\n",
    "ax2.axhline(y=95, color='g', linestyle='--', linewidth=2, label='95% variance threshold')\n",
    "ax2.axvline(x=30, color='orange', linestyle=':', linewidth=2, label='30 components (optimal)')\n",
    "ax2.set_xlabel('Number of Principal Components', fontsize=12)\n",
    "ax2.set_ylabel('Cumulative Explained Variance (%)', fontsize=12)\n",
    "ax2.set_title('Cumulative Explained Variance - MNIST Dataset', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(0, 65)\n",
    "ax2.set_ylim([0, 105])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Scree plot and cumulative variance visualization displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74f9e25",
   "metadata": {},
   "source": [
    "## Section 6: Classification with Multiple PCA Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dda7b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree classifier with different number of PCA components\n",
    "components_to_test = [5, 10, 15, 20, 30, 50, 64]\n",
    "results = {}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CLASSIFICATION PERFORMANCE WITH DIFFERENT PCA COMPONENTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for n_comp in components_to_test:\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train Decision Tree classifier\n",
    "    dt = DecisionTreeClassifier(max_depth=20, random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = dt.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[n_comp] = accuracy\n",
    "    \n",
    "    print(f\"Components: {n_comp:2d} | Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9fbd82",
   "metadata": {},
   "source": [
    "## Section 7: Plot Accuracy Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4458e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy vs components\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(list(results.keys()), list(results.values()), 'bo-', linewidth=3, markersize=10, label='Accuracy')\n",
    "plt.axvline(x=30, color='red', linestyle='--', linewidth=2, label='Optimal (30 components)')\n",
    "plt.xlabel('Number of Principal Components', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Classification Accuracy', fontsize=12, fontweight='bold')\n",
    "plt.title('Decision Tree Classification Accuracy vs PCA Components', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=11)\n",
    "plt.xticks(components_to_test)\n",
    "plt.ylim([0.9, 1.0])\n",
    "\n",
    "# Add value labels on points\n",
    "for comp, acc in results.items():\n",
    "    plt.annotate(f'{acc:.3f}', xy=(comp, acc), xytext=(0, 10), \n",
    "                textcoords='offset points', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Accuracy plot displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c78d15",
   "metadata": {},
   "source": [
    "## Section 8: Detailed Classification with Optimal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048d8ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use optimal number of components (30 - good balance)\n",
    "n_optimal = 30\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"DETAILED CLASSIFICATION WITH {n_optimal} OPTIMAL COMPONENTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Apply PCA with optimal components\n",
    "pca_optimal = PCA(n_components=n_optimal)\n",
    "X_pca_optimal = pca_optimal.fit_transform(X_scaled)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca_optimal, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Decision Tree\n",
    "dt_optimal = DecisionTreeClassifier(max_depth=20, random_state=42)\n",
    "dt_optimal.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_optimal = dt_optimal.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_optimal = accuracy_score(y_test, y_pred_optimal)\n",
    "print(f\"\\nAccuracy Score: {accuracy_optimal:.4f} ({accuracy_optimal*100:.2f}%)\")\n",
    "print(f\"Number of test samples: {len(y_test)}\")\n",
    "print(f\"Correct predictions: {np.sum(y_pred_optimal == y_test)}\")\n",
    "print(f\"Incorrect predictions: {np.sum(y_pred_optimal != y_test)}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_optimal)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_optimal, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bb1d07",
   "metadata": {},
   "source": [
    "## Section 9: Display Confusion Matrix Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9f225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Confusion Matrix as a heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, \n",
    "            xticklabels=range(10), yticklabels=range(10),\n",
    "            cbar_kws={'label': 'Number of Samples'},\n",
    "            annot_kws={'size': 11, 'weight': 'bold'})\n",
    "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "plt.title(f'Confusion Matrix - MNIST with {n_optimal} PCA Components\\nAccuracy: {accuracy_optimal:.4f}', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Confusion matrix heatmap displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae48b7b",
   "metadata": {},
   "source": [
    "## Section 10: Visualize Principal Components as Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f9d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some principal components as images\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VISUALIZATION OF PRINCIPAL COMPONENTS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Each principal component is reshaped to an 8x8 image\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    # Reshape component to 8x8 image\n",
    "    component_image = pca_full.components_[i].reshape(8, 8)\n",
    "    axes[i].imshow(component_image, cmap='gray')\n",
    "    axes[i].set_title(f'PC{i+1}\\nVariance: {explained_variance_ratio[i]*100:.2f}%', fontsize=11, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('First 10 Principal Components Visualization', fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Principal components visualization displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da997a3",
   "metadata": {},
   "source": [
    "## Section 11: Compare Original vs Reconstructed Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205df7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original digits and their reconstruction with optimal PCA\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ORIGINAL vs RECONSTRUCTED DIGITS COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Reconstruction using {n_optimal} principal components\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 10, figsize=(18, 8))\n",
    "\n",
    "# Visualize first 10 samples\n",
    "for idx in range(10):\n",
    "    # Original image\n",
    "    original_image = X[idx].reshape(8, 8)\n",
    "    axes[0, idx].imshow(original_image, cmap='gray')\n",
    "    axes[0, idx].set_title(f'{y[idx]}', fontsize=11, fontweight='bold')\n",
    "    axes[0, idx].axis('off')\n",
    "    if idx == 0:\n",
    "        axes[0, idx].set_ylabel('Original', fontsize=12, fontweight='bold', rotation=0, ha='right', va='center')\n",
    "    \n",
    "    # PCA transformed and reconstructed\n",
    "    pca_reconstructed = pca_optimal.inverse_transform(pca_optimal.transform(X_scaled[idx:idx+1]))\n",
    "    reconstructed_image = pca_reconstructed[0].reshape(8, 8)\n",
    "    axes[1, idx].imshow(reconstructed_image, cmap='gray')\n",
    "    axes[1, idx].axis('off')\n",
    "    if idx == 0:\n",
    "        axes[1, idx].set_ylabel('Reconstructed', fontsize=12, fontweight='bold', rotation=0, ha='right', va='center')\n",
    "    \n",
    "    # Difference visualization\n",
    "    difference = np.abs(original_image - reconstructed_image)\n",
    "    axes[2, idx].imshow(difference, cmap='hot')\n",
    "    axes[2, idx].axis('off')\n",
    "    if idx == 0:\n",
    "        axes[2, idx].set_ylabel('Difference', fontsize=12, fontweight='bold', rotation=0, ha='right', va='center')\n",
    "\n",
    "plt.suptitle(f'Digit Reconstruction with {n_optimal} PCA Components\\n(Original → Reconstructed → | Difference |)', \n",
    "             fontsize=14, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Reconstruction comparison visualization displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e85e73a",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
